{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14개의 feature 모두 넣고 ACC구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import os, sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import timeit\n",
    "import datetime\n",
    "from scipy import interp\n",
    "\n",
    "import keras\n",
    "from keras import callbacks\n",
    "from keras import losses\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, BatchNormalization, Dropout\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score  # plot_roc_curve\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sonyalib as sonya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "        return optimizer.lr\n",
    "    return lr\n",
    "\n",
    "\n",
    "def mlp_model(num_input, dropout=0, lr=0.005, l1=9, l2=9):\n",
    "    keras.backend.clear_session()\n",
    "    ## 모델 구성하기\n",
    "    model = Sequential()\n",
    "    # print learning rate\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    lr_metric = get_lr_metric(optimizer)\n",
    "\n",
    "    model.add(Dense(l1, activation='relu', input_dim=num_input, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(l2, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer='he_normal'))\n",
    "\n",
    "    ## 모델 컴파일\n",
    "    model.compile(optimizer=Adam(lr), loss=losses.binary_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "    # model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model, X, y, nfold=5, nbatch=5, nlr=0.001, l1=16, l2=16):\n",
    "    global record_count\n",
    "    kfold = KFold(n_splits=nfold, shuffle=True)\n",
    "    accuracy = []\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    # K-fold cross validation\n",
    "    # 학습 데이터를 이용해서 학습\n",
    "\n",
    "    i = 1\n",
    "    for train_index, validation_index in kfold.split(X, y):\n",
    "\n",
    "        kX_train, kX_test = X.iloc[train_index], X.iloc[validation_index]\n",
    "        ky_train, ky_test = y.iloc[train_index], y.iloc[validation_index]\n",
    "\n",
    "        print(\"======================batch: {}, lr = {}, FOLD: {}====================\".format(nbatch, nlr, i))\n",
    "        cbks = [callbacks.LearningRateScheduler(lambda epoch: 0.001 * 0.5 ** (epoch // 2)),\n",
    "                callbacks.TensorBoard(write_graph=False)]\n",
    "        # hist = model.fit(kX_train, ky_train, epochs=500, batch_size=5, validation_data=(kX_test,ky_test),callbacks=[tb_hist])\n",
    "        model.fit(kX_train, ky_train, epochs=500, batch_size=nbatch, validation_data=(kX_test, ky_test), callbacks=cbks, verbose=2)\n",
    "        # model.save('brc_mlp_model.h5')\n",
    "\n",
    "        y_val_cat_prob = model.predict_proba(kX_test)\n",
    "\n",
    "        k_accuracy = '%.4f' % (model.evaluate(kX_test, ky_test)[1])\n",
    "        accuracy.append(k_accuracy)\n",
    "\n",
    "        # roc curve\n",
    "        fpr, tpr, t = roc_curve(y.iloc[validation_index], y_val_cat_prob)\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "        # final_lr = model.optimizer.lr\n",
    "        plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.2f) ' % (i, roc_auc))\n",
    "        i = i + 1\n",
    "\n",
    "        \n",
    "    # 전체 검증 결과 출력\n",
    "    print('\\nK-fold cross validation Accuracy: {}'.format(accuracy))\n",
    "    # print('\\nK-fold cross validation mean Accuracy: {}'.format(np.mean(accuracy)))\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    file_name = str(\"{}b{}_lr{}_1st{}_2nd{}\".format(current_time, nbatch, nlr, l1, l2))\n",
    "    #\n",
    "    # ## 모델저장\n",
    "    # model_json = model.to_json()\n",
    "    # # if os.path.isfile(\".model/{}.json\".format(file_name)):\n",
    "    # with open(\".model/{}.json\".format(file_name), \"x\") as json_file:\n",
    "    #     print(\"saved model to disk\")\n",
    "    #     json_file.write(model_json)\n",
    "    # # else:\n",
    "    #\n",
    "    #\n",
    "    # ## weights 저장\n",
    "    # model.save_weights(\".model/{}.h5\".format(file_name))\n",
    "    # print(\"saved model weights to disk\")\n",
    "    #\n",
    "\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, color='blue', label=r'Mean ROC (AUC = %0.2f )' % (mean_auc), lw=2, alpha=1)\n",
    "   \n",
    "    font1 = {'family': 'serif',\n",
    "             'color': 'darkred',\n",
    "             'weight': 'normal',\n",
    "             'size': 10}\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC(batch: {}, lr: {}, l1: {}, l2: {})'.format(nbatch, nlr, l1, l2))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.show()\n",
    "    record_count = record_count + 1\n",
    "    return mean_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         cT   hu_diff  homogeneous    cN\n",
      "0    0.2000  0.611257         1.00  0.00\n",
      "1    0.1375  0.237244         1.00  0.00\n",
      "2    0.1875  0.448185         1.00  0.00\n",
      "3    0.1125  0.542346         0.75  0.25\n",
      "4    0.1875  0.401368         0.50  0.25\n",
      "..      ...       ...          ...   ...\n",
      "275  0.3125  0.469227         0.00  0.50\n",
      "276  0.0875  0.419253         0.75  0.75\n",
      "277  0.1375  0.290900         0.75  0.00\n",
      "278  0.2250  0.375592         1.00  0.25\n",
      "279  0.0750  0.290373         0.50  0.00\n",
      "\n",
      "[280 rows x 4 columns]\n",
      "['cT', 'hu_diff', 'homogeneous', 'cN']\n",
      "======================batch: 20, lr = 0.001, FOLD: 1====================\n",
      "Train on 156 samples, validate on 40 samples\n",
      "Epoch 1/500\n",
      " - 0s - loss: 1.0165 - accuracy: 0.4295 - val_loss: 1.1053 - val_accuracy: 0.3750\n",
      "Epoch 2/500\n",
      " - 0s - loss: 0.9422 - accuracy: 0.4744 - val_loss: 1.0341 - val_accuracy: 0.3750\n",
      "Epoch 3/500\n",
      " - 0s - loss: 0.9294 - accuracy: 0.4872 - val_loss: 0.9887 - val_accuracy: 0.4000\n",
      "Epoch 4/500\n",
      " - 0s - loss: 0.8728 - accuracy: 0.5449 - val_loss: 0.9480 - val_accuracy: 0.4250\n",
      "Epoch 5/500\n",
      " - 0s - loss: 0.8567 - accuracy: 0.5256 - val_loss: 0.9171 - val_accuracy: 0.4250\n",
      "Epoch 6/500\n",
      " - 0s - loss: 0.8737 - accuracy: 0.5256 - val_loss: 0.8902 - val_accuracy: 0.4500\n",
      "Epoch 7/500\n",
      " - 0s - loss: 0.8402 - accuracy: 0.5833 - val_loss: 0.8676 - val_accuracy: 0.4750\n",
      "Epoch 8/500\n",
      " - 0s - loss: 0.8549 - accuracy: 0.5449 - val_loss: 0.8478 - val_accuracy: 0.4500\n",
      "Epoch 9/500\n",
      " - 0s - loss: 0.8643 - accuracy: 0.5385 - val_loss: 0.8324 - val_accuracy: 0.4500\n",
      "Epoch 10/500\n",
      " - 0s - loss: 0.8440 - accuracy: 0.5513 - val_loss: 0.8186 - val_accuracy: 0.4250\n",
      "Epoch 11/500\n",
      " - 0s - loss: 0.8176 - accuracy: 0.5385 - val_loss: 0.8067 - val_accuracy: 0.5000\n",
      "Epoch 12/500\n",
      " - 0s - loss: 0.8319 - accuracy: 0.5385 - val_loss: 0.7960 - val_accuracy: 0.5000\n",
      "Epoch 13/500\n",
      " - 0s - loss: 0.8238 - accuracy: 0.5064 - val_loss: 0.7865 - val_accuracy: 0.5000\n",
      "Epoch 14/500\n",
      " - 0s - loss: 0.8355 - accuracy: 0.5513 - val_loss: 0.7775 - val_accuracy: 0.4750\n",
      "Epoch 15/500\n",
      " - 0s - loss: 0.8288 - accuracy: 0.5513 - val_loss: 0.7696 - val_accuracy: 0.5000\n",
      "Epoch 16/500\n",
      " - 0s - loss: 0.8370 - accuracy: 0.5705 - val_loss: 0.7625 - val_accuracy: 0.5000\n",
      "Epoch 17/500\n",
      " - 0s - loss: 0.8780 - accuracy: 0.5000 - val_loss: 0.7562 - val_accuracy: 0.5000\n",
      "Epoch 18/500\n",
      " - 0s - loss: 0.8278 - accuracy: 0.5385 - val_loss: 0.7502 - val_accuracy: 0.4750\n",
      "Epoch 19/500\n",
      " - 0s - loss: 0.8506 - accuracy: 0.5128 - val_loss: 0.7440 - val_accuracy: 0.5000\n",
      "Epoch 20/500\n",
      " - 0s - loss: 0.8280 - accuracy: 0.5769 - val_loss: 0.7381 - val_accuracy: 0.5000\n",
      "Epoch 21/500\n",
      " - 0s - loss: 0.8192 - accuracy: 0.5577 - val_loss: 0.7327 - val_accuracy: 0.4750\n",
      "Epoch 22/500\n",
      " - 0s - loss: 0.8278 - accuracy: 0.5192 - val_loss: 0.7281 - val_accuracy: 0.4750\n",
      "Epoch 23/500\n",
      " - 0s - loss: 0.8191 - accuracy: 0.5513 - val_loss: 0.7241 - val_accuracy: 0.4750\n",
      "Epoch 24/500\n",
      " - 0s - loss: 0.8383 - accuracy: 0.5577 - val_loss: 0.7216 - val_accuracy: 0.4750\n",
      "Epoch 25/500\n",
      " - 0s - loss: 0.8327 - accuracy: 0.5833 - val_loss: 0.7190 - val_accuracy: 0.4750\n",
      "Epoch 26/500\n",
      " - 0s - loss: 0.8449 - accuracy: 0.5833 - val_loss: 0.7171 - val_accuracy: 0.4750\n",
      "Epoch 27/500\n",
      " - 0s - loss: 0.8176 - accuracy: 0.5256 - val_loss: 0.7149 - val_accuracy: 0.4750\n",
      "Epoch 28/500\n",
      " - 0s - loss: 0.8309 - accuracy: 0.5385 - val_loss: 0.7133 - val_accuracy: 0.4500\n",
      "Epoch 29/500\n",
      " - 0s - loss: 0.8421 - accuracy: 0.5449 - val_loss: 0.7116 - val_accuracy: 0.4500\n",
      "Epoch 30/500\n",
      " - 0s - loss: 0.8157 - accuracy: 0.5449 - val_loss: 0.7102 - val_accuracy: 0.4500\n",
      "Epoch 31/500\n",
      " - 0s - loss: 0.8007 - accuracy: 0.5385 - val_loss: 0.7093 - val_accuracy: 0.4500\n",
      "Epoch 32/500\n",
      " - 0s - loss: 0.8143 - accuracy: 0.5705 - val_loss: 0.7081 - val_accuracy: 0.4750\n",
      "Epoch 33/500\n",
      " - 0s - loss: 0.8285 - accuracy: 0.5769 - val_loss: 0.7072 - val_accuracy: 0.4750\n",
      "Epoch 34/500\n",
      " - 0s - loss: 0.8500 - accuracy: 0.5897 - val_loss: 0.7069 - val_accuracy: 0.4750\n",
      "Epoch 35/500\n",
      " - 0s - loss: 0.8175 - accuracy: 0.5385 - val_loss: 0.7056 - val_accuracy: 0.4750\n",
      "Epoch 36/500\n",
      " - 0s - loss: 0.8526 - accuracy: 0.5000 - val_loss: 0.7053 - val_accuracy: 0.4750\n",
      "Epoch 37/500\n",
      " - 0s - loss: 0.8217 - accuracy: 0.5769 - val_loss: 0.7048 - val_accuracy: 0.4750\n",
      "Epoch 38/500\n",
      " - 0s - loss: 0.8238 - accuracy: 0.5769 - val_loss: 0.7040 - val_accuracy: 0.5000\n",
      "Epoch 39/500\n",
      " - 0s - loss: 0.8117 - accuracy: 0.5769 - val_loss: 0.7036 - val_accuracy: 0.5250\n",
      "Epoch 40/500\n",
      " - 0s - loss: 0.8127 - accuracy: 0.5513 - val_loss: 0.7040 - val_accuracy: 0.5250\n",
      "Epoch 41/500\n",
      " - 0s - loss: 0.8519 - accuracy: 0.5128 - val_loss: 0.7037 - val_accuracy: 0.5250\n",
      "Epoch 42/500\n",
      " - 0s - loss: 0.8416 - accuracy: 0.5641 - val_loss: 0.7040 - val_accuracy: 0.5250\n",
      "Epoch 43/500\n",
      " - 0s - loss: 0.8181 - accuracy: 0.5449 - val_loss: 0.7039 - val_accuracy: 0.5500\n",
      "Epoch 44/500\n",
      " - 0s - loss: 0.8392 - accuracy: 0.5064 - val_loss: 0.7043 - val_accuracy: 0.5500\n",
      "Epoch 45/500\n",
      " - 0s - loss: 0.8199 - accuracy: 0.5449 - val_loss: 0.7053 - val_accuracy: 0.5500\n",
      "Epoch 46/500\n",
      " - 0s - loss: 0.7945 - accuracy: 0.5577 - val_loss: 0.7059 - val_accuracy: 0.5500\n",
      "Epoch 47/500\n",
      " - 0s - loss: 0.8371 - accuracy: 0.5577 - val_loss: 0.7066 - val_accuracy: 0.5750\n",
      "Epoch 48/500\n",
      " - 0s - loss: 0.8431 - accuracy: 0.5513 - val_loss: 0.7080 - val_accuracy: 0.5750\n",
      "Epoch 49/500\n",
      " - 0s - loss: 0.8486 - accuracy: 0.5577 - val_loss: 0.7094 - val_accuracy: 0.5750\n",
      "Epoch 50/500\n",
      " - 0s - loss: 0.8288 - accuracy: 0.5897 - val_loss: 0.7107 - val_accuracy: 0.5500\n",
      "Epoch 51/500\n",
      " - 0s - loss: 0.8364 - accuracy: 0.5449 - val_loss: 0.7121 - val_accuracy: 0.5500\n",
      "Epoch 52/500\n",
      " - 0s - loss: 0.8262 - accuracy: 0.5513 - val_loss: 0.7136 - val_accuracy: 0.5500\n",
      "Epoch 53/500\n",
      " - 0s - loss: 0.8552 - accuracy: 0.5513 - val_loss: 0.7148 - val_accuracy: 0.5500\n",
      "Epoch 54/500\n",
      " - 0s - loss: 0.8517 - accuracy: 0.5128 - val_loss: 0.7167 - val_accuracy: 0.5500\n",
      "Epoch 55/500\n",
      " - 0s - loss: 0.8428 - accuracy: 0.5577 - val_loss: 0.7185 - val_accuracy: 0.5500\n",
      "Epoch 56/500\n",
      " - 0s - loss: 0.8147 - accuracy: 0.5641 - val_loss: 0.7206 - val_accuracy: 0.5500\n",
      "Epoch 57/500\n",
      " - 0s - loss: 0.8011 - accuracy: 0.5897 - val_loss: 0.7230 - val_accuracy: 0.5500\n",
      "Epoch 58/500\n",
      " - 0s - loss: 0.8158 - accuracy: 0.5321 - val_loss: 0.7253 - val_accuracy: 0.5500\n",
      "Epoch 59/500\n",
      " - 0s - loss: 0.8478 - accuracy: 0.5513 - val_loss: 0.7276 - val_accuracy: 0.5500\n",
      "Epoch 60/500\n",
      " - 0s - loss: 0.8100 - accuracy: 0.5449 - val_loss: 0.7301 - val_accuracy: 0.5250\n",
      "Epoch 61/500\n",
      " - 0s - loss: 0.8360 - accuracy: 0.5321 - val_loss: 0.7328 - val_accuracy: 0.5500\n",
      "Epoch 62/500\n",
      " - 0s - loss: 0.8454 - accuracy: 0.5513 - val_loss: 0.7362 - val_accuracy: 0.5250\n",
      "Epoch 63/500\n",
      " - 0s - loss: 0.8503 - accuracy: 0.5577 - val_loss: 0.7388 - val_accuracy: 0.5500\n",
      "Epoch 64/500\n",
      " - 0s - loss: 0.8248 - accuracy: 0.5833 - val_loss: 0.7416 - val_accuracy: 0.5500\n",
      "Epoch 65/500\n",
      " - 0s - loss: 0.8293 - accuracy: 0.5705 - val_loss: 0.7437 - val_accuracy: 0.5500\n",
      "Epoch 66/500\n",
      " - 0s - loss: 0.8425 - accuracy: 0.5321 - val_loss: 0.7467 - val_accuracy: 0.5500\n",
      "Epoch 67/500\n",
      " - 0s - loss: 0.8461 - accuracy: 0.5385 - val_loss: 0.7491 - val_accuracy: 0.5500\n",
      "Epoch 68/500\n",
      " - 0s - loss: 0.8341 - accuracy: 0.5256 - val_loss: 0.7515 - val_accuracy: 0.5500\n",
      "Epoch 69/500\n",
      " - 0s - loss: 0.8194 - accuracy: 0.5128 - val_loss: 0.7543 - val_accuracy: 0.5500\n",
      "Epoch 70/500\n",
      " - 0s - loss: 0.8491 - accuracy: 0.5577 - val_loss: 0.7569 - val_accuracy: 0.5500\n",
      "Epoch 71/500\n",
      " - 0s - loss: 0.8541 - accuracy: 0.5321 - val_loss: 0.7586 - val_accuracy: 0.5750\n",
      "Epoch 72/500\n",
      " - 0s - loss: 0.8185 - accuracy: 0.5641 - val_loss: 0.7604 - val_accuracy: 0.5750\n",
      "Epoch 73/500\n",
      " - 0s - loss: 0.8187 - accuracy: 0.5769 - val_loss: 0.7627 - val_accuracy: 0.5750\n",
      "Epoch 74/500\n",
      " - 0s - loss: 0.8207 - accuracy: 0.5449 - val_loss: 0.7648 - val_accuracy: 0.5750\n",
      "Epoch 75/500\n",
      " - 0s - loss: 0.8427 - accuracy: 0.5705 - val_loss: 0.7666 - val_accuracy: 0.5750\n",
      "Epoch 76/500\n",
      " - 0s - loss: 0.8157 - accuracy: 0.5321 - val_loss: 0.7697 - val_accuracy: 0.5750\n",
      "Epoch 77/500\n",
      " - 0s - loss: 0.8456 - accuracy: 0.5513 - val_loss: 0.7712 - val_accuracy: 0.5750\n",
      "Epoch 78/500\n",
      " - 0s - loss: 0.8668 - accuracy: 0.5256 - val_loss: 0.7732 - val_accuracy: 0.5750\n",
      "Epoch 79/500\n",
      " - 0s - loss: 0.8411 - accuracy: 0.5385 - val_loss: 0.7737 - val_accuracy: 0.5750\n",
      "Epoch 80/500\n",
      " - 0s - loss: 0.8092 - accuracy: 0.5769 - val_loss: 0.7753 - val_accuracy: 0.5750\n",
      "Epoch 81/500\n",
      " - 0s - loss: 0.8117 - accuracy: 0.5577 - val_loss: 0.7761 - val_accuracy: 0.5750\n",
      "Epoch 82/500\n",
      " - 0s - loss: 0.8420 - accuracy: 0.5321 - val_loss: 0.7770 - val_accuracy: 0.5750\n",
      "Epoch 83/500\n",
      " - 0s - loss: 0.8047 - accuracy: 0.5641 - val_loss: 0.7781 - val_accuracy: 0.5750\n",
      "Epoch 84/500\n",
      " - 0s - loss: 0.8361 - accuracy: 0.5577 - val_loss: 0.7798 - val_accuracy: 0.5750\n",
      "Epoch 85/500\n",
      " - 0s - loss: 0.8438 - accuracy: 0.5513 - val_loss: 0.7801 - val_accuracy: 0.5750\n",
      "Epoch 86/500\n",
      " - 0s - loss: 0.8348 - accuracy: 0.5449 - val_loss: 0.7804 - val_accuracy: 0.5750\n",
      "Epoch 87/500\n",
      " - 0s - loss: 0.8263 - accuracy: 0.5641 - val_loss: 0.7812 - val_accuracy: 0.5750\n",
      "Epoch 88/500\n",
      " - 0s - loss: 0.8429 - accuracy: 0.5321 - val_loss: 0.7825 - val_accuracy: 0.5750\n",
      "Epoch 89/500\n",
      " - 0s - loss: 0.8237 - accuracy: 0.5513 - val_loss: 0.7835 - val_accuracy: 0.5750\n",
      "Epoch 90/500\n",
      " - 0s - loss: 0.8330 - accuracy: 0.5577 - val_loss: 0.7848 - val_accuracy: 0.5750\n",
      "Epoch 91/500\n",
      " - 0s - loss: 0.8430 - accuracy: 0.5641 - val_loss: 0.7862 - val_accuracy: 0.5750\n",
      "Epoch 92/500\n",
      " - 0s - loss: 0.8077 - accuracy: 0.5513 - val_loss: 0.7869 - val_accuracy: 0.5500\n",
      "Epoch 93/500\n",
      " - 0s - loss: 0.8434 - accuracy: 0.5449 - val_loss: 0.7875 - val_accuracy: 0.5750\n",
      "Epoch 94/500\n",
      " - 0s - loss: 0.8149 - accuracy: 0.5833 - val_loss: 0.7881 - val_accuracy: 0.5750\n",
      "Epoch 95/500\n",
      " - 0s - loss: 0.8192 - accuracy: 0.5705 - val_loss: 0.7879 - val_accuracy: 0.5750\n",
      "Epoch 96/500\n",
      " - 0s - loss: 0.8266 - accuracy: 0.5641 - val_loss: 0.7873 - val_accuracy: 0.5500\n",
      "Epoch 97/500\n",
      " - 0s - loss: 0.8480 - accuracy: 0.5064 - val_loss: 0.7880 - val_accuracy: 0.5500\n",
      "Epoch 98/500\n",
      " - 0s - loss: 0.8439 - accuracy: 0.5513 - val_loss: 0.7886 - val_accuracy: 0.5500\n",
      "Epoch 99/500\n",
      " - 0s - loss: 0.8198 - accuracy: 0.5641 - val_loss: 0.7892 - val_accuracy: 0.5500\n",
      "Epoch 100/500\n",
      " - 0s - loss: 0.8141 - accuracy: 0.5321 - val_loss: 0.7886 - val_accuracy: 0.5500\n",
      "Epoch 101/500\n",
      " - 0s - loss: 0.8054 - accuracy: 0.5962 - val_loss: 0.7892 - val_accuracy: 0.5500\n",
      "Epoch 102/500\n",
      " - 0s - loss: 0.8506 - accuracy: 0.5321 - val_loss: 0.7889 - val_accuracy: 0.5500\n",
      "Epoch 103/500\n",
      " - 0s - loss: 0.8233 - accuracy: 0.5449 - val_loss: 0.7893 - val_accuracy: 0.5500\n",
      "Epoch 104/500\n",
      " - 0s - loss: 0.8280 - accuracy: 0.5641 - val_loss: 0.7899 - val_accuracy: 0.5500\n",
      "Epoch 105/500\n",
      " - 0s - loss: 0.8444 - accuracy: 0.5449 - val_loss: 0.7901 - val_accuracy: 0.5500\n",
      "Epoch 106/500\n",
      " - 0s - loss: 0.8397 - accuracy: 0.5256 - val_loss: 0.7903 - val_accuracy: 0.5500\n",
      "Epoch 107/500\n",
      " - 0s - loss: 0.8466 - accuracy: 0.5321 - val_loss: 0.7899 - val_accuracy: 0.5500\n",
      "Epoch 108/500\n",
      " - 0s - loss: 0.8282 - accuracy: 0.5641 - val_loss: 0.7901 - val_accuracy: 0.5500\n",
      "Epoch 109/500\n",
      " - 0s - loss: 0.8111 - accuracy: 0.5321 - val_loss: 0.7911 - val_accuracy: 0.5500\n",
      "Epoch 110/500\n",
      " - 0s - loss: 0.8412 - accuracy: 0.5641 - val_loss: 0.7914 - val_accuracy: 0.5500\n",
      "Epoch 111/500\n",
      " - 0s - loss: 0.7935 - accuracy: 0.5833 - val_loss: 0.7918 - val_accuracy: 0.5500\n",
      "Epoch 112/500\n",
      " - 0s - loss: 0.8184 - accuracy: 0.5833 - val_loss: 0.7920 - val_accuracy: 0.5500\n",
      "Epoch 113/500\n",
      " - 0s - loss: 0.8442 - accuracy: 0.5256 - val_loss: 0.7916 - val_accuracy: 0.5500\n",
      "Epoch 114/500\n",
      " - 0s - loss: 0.8494 - accuracy: 0.5385 - val_loss: 0.7917 - val_accuracy: 0.5500\n",
      "Epoch 115/500\n",
      " - 0s - loss: 0.7829 - accuracy: 0.5705 - val_loss: 0.7909 - val_accuracy: 0.5500\n",
      "Epoch 116/500\n",
      " - 0s - loss: 0.8119 - accuracy: 0.5705 - val_loss: 0.7906 - val_accuracy: 0.5500\n",
      "Epoch 117/500\n",
      " - 0s - loss: 0.8015 - accuracy: 0.5321 - val_loss: 0.7914 - val_accuracy: 0.5500\n",
      "Epoch 118/500\n",
      " - 0s - loss: 0.8231 - accuracy: 0.5769 - val_loss: 0.7913 - val_accuracy: 0.5500\n",
      "Epoch 119/500\n",
      " - 0s - loss: 0.8493 - accuracy: 0.5321 - val_loss: 0.7917 - val_accuracy: 0.5500\n",
      "Epoch 120/500\n",
      " - 0s - loss: 0.8185 - accuracy: 0.5513 - val_loss: 0.7918 - val_accuracy: 0.5500\n",
      "Epoch 121/500\n",
      " - 0s - loss: 0.8452 - accuracy: 0.5577 - val_loss: 0.7916 - val_accuracy: 0.5500\n",
      "Epoch 122/500\n",
      " - 0s - loss: 0.8508 - accuracy: 0.5513 - val_loss: 0.7913 - val_accuracy: 0.5500\n",
      "Epoch 123/500\n",
      " - 0s - loss: 0.8437 - accuracy: 0.5449 - val_loss: 0.7910 - val_accuracy: 0.5500\n",
      "Epoch 124/500\n",
      " - 0s - loss: 0.8179 - accuracy: 0.5385 - val_loss: 0.7923 - val_accuracy: 0.5500\n",
      "Epoch 125/500\n",
      " - 0s - loss: 0.8218 - accuracy: 0.5577 - val_loss: 0.7912 - val_accuracy: 0.5500\n",
      "Epoch 126/500\n",
      " - 0s - loss: 0.8313 - accuracy: 0.5641 - val_loss: 0.7919 - val_accuracy: 0.5500\n",
      "Epoch 127/500\n",
      " - 0s - loss: 0.8365 - accuracy: 0.5513 - val_loss: 0.7905 - val_accuracy: 0.5500\n",
      "Epoch 128/500\n",
      " - 0s - loss: 0.8386 - accuracy: 0.5192 - val_loss: 0.7909 - val_accuracy: 0.5500\n",
      "Epoch 129/500\n",
      " - 0s - loss: 0.8153 - accuracy: 0.5385 - val_loss: 0.7923 - val_accuracy: 0.5500\n",
      "Epoch 130/500\n",
      " - 0s - loss: 0.8501 - accuracy: 0.5513 - val_loss: 0.7932 - val_accuracy: 0.5500\n",
      "Epoch 131/500\n",
      " - 0s - loss: 0.8530 - accuracy: 0.5449 - val_loss: 0.7926 - val_accuracy: 0.5500\n",
      "Epoch 132/500\n",
      " - 0s - loss: 0.8108 - accuracy: 0.5705 - val_loss: 0.7925 - val_accuracy: 0.5500\n",
      "Epoch 133/500\n",
      " - 0s - loss: 0.8362 - accuracy: 0.5128 - val_loss: 0.7921 - val_accuracy: 0.5500\n",
      "Epoch 134/500\n",
      " - 0s - loss: 0.8518 - accuracy: 0.5192 - val_loss: 0.7927 - val_accuracy: 0.5500\n",
      "Epoch 135/500\n",
      " - 0s - loss: 0.8518 - accuracy: 0.5641 - val_loss: 0.7929 - val_accuracy: 0.5500\n",
      "Epoch 136/500\n",
      " - 0s - loss: 0.8301 - accuracy: 0.5449 - val_loss: 0.7927 - val_accuracy: 0.5500\n",
      "Epoch 137/500\n",
      " - 0s - loss: 0.8310 - accuracy: 0.5513 - val_loss: 0.7920 - val_accuracy: 0.5500\n",
      "Epoch 138/500\n",
      " - 0s - loss: 0.8383 - accuracy: 0.5321 - val_loss: 0.7919 - val_accuracy: 0.5500\n",
      "Epoch 139/500\n",
      " - 0s - loss: 0.8313 - accuracy: 0.5321 - val_loss: 0.7922 - val_accuracy: 0.5500\n",
      "Epoch 140/500\n",
      " - 0s - loss: 0.8353 - accuracy: 0.5577 - val_loss: 0.7928 - val_accuracy: 0.5500\n",
      "Epoch 141/500\n",
      " - 0s - loss: 0.8057 - accuracy: 0.5705 - val_loss: 0.7935 - val_accuracy: 0.5500\n",
      "Epoch 142/500\n",
      " - 0s - loss: 0.8340 - accuracy: 0.5449 - val_loss: 0.7934 - val_accuracy: 0.5500\n",
      "Epoch 143/500\n",
      " - 0s - loss: 0.8386 - accuracy: 0.5385 - val_loss: 0.7937 - val_accuracy: 0.5500\n",
      "Epoch 144/500\n",
      " - 0s - loss: 0.8326 - accuracy: 0.5449 - val_loss: 0.7930 - val_accuracy: 0.5500\n",
      "Epoch 145/500\n",
      " - 0s - loss: 0.8260 - accuracy: 0.5705 - val_loss: 0.7930 - val_accuracy: 0.5500\n",
      "Epoch 146/500\n",
      " - 0s - loss: 0.8273 - accuracy: 0.5513 - val_loss: 0.7940 - val_accuracy: 0.5500\n",
      "Epoch 147/500\n",
      " - 0s - loss: 0.8365 - accuracy: 0.5769 - val_loss: 0.7942 - val_accuracy: 0.5500\n",
      "Epoch 148/500\n",
      " - 0s - loss: 0.8133 - accuracy: 0.5833 - val_loss: 0.7939 - val_accuracy: 0.5500\n",
      "Epoch 149/500\n",
      " - 0s - loss: 0.8385 - accuracy: 0.5256 - val_loss: 0.7935 - val_accuracy: 0.5500\n",
      "Epoch 150/500\n",
      " - 0s - loss: 0.8417 - accuracy: 0.5321 - val_loss: 0.7938 - val_accuracy: 0.5500\n",
      "Epoch 151/500\n",
      " - 0s - loss: 0.8034 - accuracy: 0.5705 - val_loss: 0.7930 - val_accuracy: 0.5500\n",
      "Epoch 152/500\n",
      " - 0s - loss: 0.8034 - accuracy: 0.5577 - val_loss: 0.7921 - val_accuracy: 0.5500\n",
      "Epoch 153/500\n",
      " - 0s - loss: 0.8459 - accuracy: 0.4936 - val_loss: 0.7925 - val_accuracy: 0.5500\n",
      "Epoch 154/500\n",
      " - 0s - loss: 0.8288 - accuracy: 0.5641 - val_loss: 0.7922 - val_accuracy: 0.5500\n",
      "Epoch 155/500\n",
      " - 0s - loss: 0.8066 - accuracy: 0.5641 - val_loss: 0.7928 - val_accuracy: 0.5500\n",
      "Epoch 156/500\n",
      " - 0s - loss: 0.8411 - accuracy: 0.4936 - val_loss: 0.7926 - val_accuracy: 0.5500\n",
      "Epoch 157/500\n",
      " - 0s - loss: 0.8444 - accuracy: 0.5513 - val_loss: 0.7930 - val_accuracy: 0.5500\n",
      "Epoch 158/500\n",
      " - 0s - loss: 0.8225 - accuracy: 0.5449 - val_loss: 0.7923 - val_accuracy: 0.5500\n",
      "Epoch 159/500\n",
      " - 0s - loss: 0.8262 - accuracy: 0.5705 - val_loss: 0.7926 - val_accuracy: 0.5500\n",
      "Epoch 160/500\n",
      " - 0s - loss: 0.8282 - accuracy: 0.5321 - val_loss: 0.7919 - val_accuracy: 0.5500\n",
      "Epoch 161/500\n",
      " - 0s - loss: 0.8108 - accuracy: 0.5705 - val_loss: 0.7921 - val_accuracy: 0.5500\n",
      "Epoch 162/500\n",
      " - 0s - loss: 0.8290 - accuracy: 0.5705 - val_loss: 0.7925 - val_accuracy: 0.5500\n",
      "Epoch 163/500\n",
      " - 0s - loss: 0.8300 - accuracy: 0.5449 - val_loss: 0.7920 - val_accuracy: 0.5500\n",
      "Epoch 164/500\n",
      " - 0s - loss: 0.8482 - accuracy: 0.5321 - val_loss: 0.7913 - val_accuracy: 0.5500\n",
      "Epoch 165/500\n",
      " - 0s - loss: 0.7988 - accuracy: 0.5962 - val_loss: 0.7913 - val_accuracy: 0.5500\n",
      "Epoch 166/500\n",
      " - 0s - loss: 0.8412 - accuracy: 0.5321 - val_loss: 0.7918 - val_accuracy: 0.5500\n",
      "Epoch 167/500\n",
      " - 0s - loss: 0.8764 - accuracy: 0.5256 - val_loss: 0.7925 - val_accuracy: 0.5500\n",
      "Epoch 168/500\n",
      " - 0s - loss: 0.8542 - accuracy: 0.5449 - val_loss: 0.7916 - val_accuracy: 0.5500\n",
      "Epoch 169/500\n",
      " - 0s - loss: 0.8189 - accuracy: 0.5769 - val_loss: 0.7916 - val_accuracy: 0.5500\n",
      "Epoch 170/500\n",
      " - 0s - loss: 0.8170 - accuracy: 0.5705 - val_loss: 0.7921 - val_accuracy: 0.5500\n",
      "Epoch 171/500\n",
      " - 0s - loss: 0.8574 - accuracy: 0.5256 - val_loss: 0.7929 - val_accuracy: 0.5500\n",
      "Epoch 172/500\n",
      " - 0s - loss: 0.8118 - accuracy: 0.5321 - val_loss: 0.7930 - val_accuracy: 0.5500\n",
      "Epoch 173/500\n",
      " - 0s - loss: 0.8362 - accuracy: 0.5449 - val_loss: 0.7922 - val_accuracy: 0.5500\n",
      "Epoch 174/500\n",
      " - 0s - loss: 0.8606 - accuracy: 0.5385 - val_loss: 0.7920 - val_accuracy: 0.5500\n",
      "Epoch 175/500\n",
      " - 0s - loss: 0.8474 - accuracy: 0.5449 - val_loss: 0.7929 - val_accuracy: 0.5500\n",
      "Epoch 176/500\n",
      " - 0s - loss: 0.7935 - accuracy: 0.5641 - val_loss: 0.7929 - val_accuracy: 0.5500\n",
      "Epoch 177/500\n",
      " - 0s - loss: 0.8444 - accuracy: 0.5192 - val_loss: 0.7945 - val_accuracy: 0.5500\n",
      "Epoch 178/500\n",
      " - 0s - loss: 0.8318 - accuracy: 0.5577 - val_loss: 0.7943 - val_accuracy: 0.5500\n",
      "Epoch 179/500\n",
      " - 0s - loss: 0.8218 - accuracy: 0.5385 - val_loss: 0.7941 - val_accuracy: 0.5500\n",
      "Epoch 180/500\n",
      " - 0s - loss: 0.8565 - accuracy: 0.5128 - val_loss: 0.7944 - val_accuracy: 0.5500\n",
      "Epoch 181/500\n",
      " - 0s - loss: 0.8516 - accuracy: 0.5577 - val_loss: 0.7945 - val_accuracy: 0.5500\n",
      "Epoch 182/500\n",
      " - 0s - loss: 0.8239 - accuracy: 0.5513 - val_loss: 0.7951 - val_accuracy: 0.5500\n",
      "Epoch 183/500\n",
      " - 0s - loss: 0.8421 - accuracy: 0.5000 - val_loss: 0.7939 - val_accuracy: 0.5500\n",
      "Epoch 184/500\n",
      " - 0s - loss: 0.7976 - accuracy: 0.5321 - val_loss: 0.7939 - val_accuracy: 0.5500\n",
      "Epoch 185/500\n",
      " - 0s - loss: 0.8495 - accuracy: 0.5449 - val_loss: 0.7942 - val_accuracy: 0.5500\n",
      "Epoch 186/500\n",
      " - 0s - loss: 0.8197 - accuracy: 0.5577 - val_loss: 0.7937 - val_accuracy: 0.5500\n",
      "Epoch 187/500\n",
      " - 0s - loss: 0.8302 - accuracy: 0.5833 - val_loss: 0.7931 - val_accuracy: 0.5500\n",
      "Epoch 188/500\n",
      " - 0s - loss: 0.8363 - accuracy: 0.5449 - val_loss: 0.7932 - val_accuracy: 0.5500\n",
      "Epoch 189/500\n",
      " - 0s - loss: 0.8038 - accuracy: 0.5769 - val_loss: 0.7923 - val_accuracy: 0.5500\n",
      "Epoch 190/500\n",
      " - 0s - loss: 0.8176 - accuracy: 0.5385 - val_loss: 0.7923 - val_accuracy: 0.5500\n",
      "Epoch 191/500\n",
      " - 0s - loss: 0.7936 - accuracy: 0.5449 - val_loss: 0.7916 - val_accuracy: 0.5500\n",
      "Epoch 192/500\n",
      " - 0s - loss: 0.8432 - accuracy: 0.5321 - val_loss: 0.7918 - val_accuracy: 0.5500\n",
      "Epoch 193/500\n",
      " - 0s - loss: 0.8373 - accuracy: 0.5577 - val_loss: 0.7919 - val_accuracy: 0.5500\n",
      "Epoch 194/500\n",
      " - 0s - loss: 0.8388 - accuracy: 0.5833 - val_loss: 0.7928 - val_accuracy: 0.5500\n",
      "Epoch 195/500\n",
      " - 0s - loss: 0.8249 - accuracy: 0.5385 - val_loss: 0.7925 - val_accuracy: 0.5500\n",
      "Epoch 196/500\n",
      " - 0s - loss: 0.8157 - accuracy: 0.5705 - val_loss: 0.7923 - val_accuracy: 0.5500\n",
      "Epoch 197/500\n",
      " - 0s - loss: 0.8350 - accuracy: 0.5449 - val_loss: 0.7925 - val_accuracy: 0.5500\n",
      "Epoch 198/500\n",
      " - 0s - loss: 0.8364 - accuracy: 0.5513 - val_loss: 0.7928 - val_accuracy: 0.5500\n",
      "Epoch 199/500\n",
      " - 0s - loss: 0.8593 - accuracy: 0.5321 - val_loss: 0.7928 - val_accuracy: 0.5500\n",
      "Epoch 200/500\n",
      " - 0s - loss: 0.8158 - accuracy: 0.5577 - val_loss: 0.7925 - val_accuracy: 0.5500\n",
      "Epoch 201/500\n",
      " - 0s - loss: 0.8218 - accuracy: 0.5577 - val_loss: 0.7923 - val_accuracy: 0.5500\n",
      "Epoch 202/500\n",
      " - 0s - loss: 0.8454 - accuracy: 0.4936 - val_loss: 0.7928 - val_accuracy: 0.5500\n",
      "Epoch 203/500\n",
      " - 0s - loss: 0.8539 - accuracy: 0.5705 - val_loss: 0.7929 - val_accuracy: 0.5500\n",
      "Epoch 204/500\n",
      " - 0s - loss: 0.8346 - accuracy: 0.5833 - val_loss: 0.7934 - val_accuracy: 0.5500\n",
      "Epoch 205/500\n",
      " - 0s - loss: 0.8178 - accuracy: 0.5833 - val_loss: 0.7933 - val_accuracy: 0.5500\n",
      "Epoch 206/500\n",
      " - 0s - loss: 0.8657 - accuracy: 0.5064 - val_loss: 0.7941 - val_accuracy: 0.5500\n",
      "Epoch 207/500\n",
      " - 0s - loss: 0.8150 - accuracy: 0.5641 - val_loss: 0.7932 - val_accuracy: 0.5500\n",
      "Epoch 208/500\n",
      " - 0s - loss: 0.8167 - accuracy: 0.5385 - val_loss: 0.7936 - val_accuracy: 0.5500\n",
      "Epoch 209/500\n",
      " - 0s - loss: 0.8545 - accuracy: 0.5449 - val_loss: 0.7938 - val_accuracy: 0.5500\n",
      "Epoch 210/500\n",
      " - 0s - loss: 0.8400 - accuracy: 0.5577 - val_loss: 0.7947 - val_accuracy: 0.5500\n",
      "Epoch 211/500\n",
      " - 0s - loss: 0.8136 - accuracy: 0.6026 - val_loss: 0.7945 - val_accuracy: 0.5500\n",
      "Epoch 212/500\n",
      " - 0s - loss: 0.7979 - accuracy: 0.5577 - val_loss: 0.7929 - val_accuracy: 0.5500\n",
      "Epoch 213/500\n",
      " - 0s - loss: 0.8546 - accuracy: 0.5321 - val_loss: 0.7932 - val_accuracy: 0.5500\n",
      "Epoch 214/500\n",
      " - 0s - loss: 0.8113 - accuracy: 0.5641 - val_loss: 0.7925 - val_accuracy: 0.5500\n",
      "Epoch 215/500\n",
      " - 0s - loss: 0.8472 - accuracy: 0.5577 - val_loss: 0.7927 - val_accuracy: 0.5500\n",
      "Epoch 216/500\n",
      " - 0s - loss: 0.8285 - accuracy: 0.5513 - val_loss: 0.7928 - val_accuracy: 0.5500\n",
      "Epoch 217/500\n",
      " - 0s - loss: 0.8050 - accuracy: 0.5641 - val_loss: 0.7918 - val_accuracy: 0.5500\n",
      "Epoch 218/500\n",
      " - 0s - loss: 0.8628 - accuracy: 0.5385 - val_loss: 0.7917 - val_accuracy: 0.5500\n",
      "Epoch 219/500\n",
      " - 0s - loss: 0.8306 - accuracy: 0.5513 - val_loss: 0.7924 - val_accuracy: 0.5500\n",
      "Epoch 220/500\n",
      " - 0s - loss: 0.8332 - accuracy: 0.5833 - val_loss: 0.7921 - val_accuracy: 0.5500\n",
      "Epoch 221/500\n",
      " - 0s - loss: 0.8285 - accuracy: 0.5385 - val_loss: 0.7929 - val_accuracy: 0.5500\n",
      "Epoch 222/500\n",
      " - 0s - loss: 0.8184 - accuracy: 0.5449 - val_loss: 0.7919 - val_accuracy: 0.5500\n",
      "Epoch 223/500\n",
      " - 0s - loss: 0.8488 - accuracy: 0.5513 - val_loss: 0.7920 - val_accuracy: 0.5500\n",
      "Epoch 224/500\n",
      " - 0s - loss: 0.8355 - accuracy: 0.5513 - val_loss: 0.7924 - val_accuracy: 0.5500\n",
      "Epoch 225/500\n",
      " - 0s - loss: 0.8428 - accuracy: 0.5192 - val_loss: 0.7925 - val_accuracy: 0.5500\n",
      "Epoch 226/500\n",
      " - 0s - loss: 0.8300 - accuracy: 0.5385 - val_loss: 0.7924 - val_accuracy: 0.5500\n",
      "Epoch 227/500\n",
      " - 0s - loss: 0.8179 - accuracy: 0.5897 - val_loss: 0.7931 - val_accuracy: 0.5500\n",
      "Epoch 228/500\n",
      " - 0s - loss: 0.8376 - accuracy: 0.5641 - val_loss: 0.7931 - val_accuracy: 0.5500\n",
      "Epoch 229/500\n",
      " - 0s - loss: 0.8117 - accuracy: 0.5192 - val_loss: 0.7931 - val_accuracy: 0.5500\n",
      "Epoch 230/500\n",
      " - 0s - loss: 0.8366 - accuracy: 0.5513 - val_loss: 0.7928 - val_accuracy: 0.5500\n",
      "Epoch 231/500\n",
      " - 0s - loss: 0.8106 - accuracy: 0.5385 - val_loss: 0.7917 - val_accuracy: 0.5500\n",
      "Epoch 232/500\n",
      " - 0s - loss: 0.8275 - accuracy: 0.5577 - val_loss: 0.7918 - val_accuracy: 0.5500\n",
      "Epoch 233/500\n",
      " - 0s - loss: 0.8427 - accuracy: 0.5385 - val_loss: 0.7922 - val_accuracy: 0.5500\n",
      "Epoch 234/500\n",
      " - 0s - loss: 0.7918 - accuracy: 0.5705 - val_loss: 0.7922 - val_accuracy: 0.5500\n",
      "Epoch 235/500\n",
      " - 0s - loss: 0.8217 - accuracy: 0.5577 - val_loss: 0.7914 - val_accuracy: 0.5500\n",
      "Epoch 236/500\n",
      " - 0s - loss: 0.8468 - accuracy: 0.5641 - val_loss: 0.7929 - val_accuracy: 0.5500\n",
      "Epoch 237/500\n",
      " - 0s - loss: 0.8084 - accuracy: 0.5513 - val_loss: 0.7936 - val_accuracy: 0.5500\n",
      "Epoch 238/500\n",
      " - 0s - loss: 0.8324 - accuracy: 0.5256 - val_loss: 0.7934 - val_accuracy: 0.5500\n",
      "Epoch 239/500\n",
      " - 0s - loss: 0.8260 - accuracy: 0.5769 - val_loss: 0.7947 - val_accuracy: 0.5500\n",
      "Epoch 240/500\n",
      " - 0s - loss: 0.8239 - accuracy: 0.5577 - val_loss: 0.7943 - val_accuracy: 0.5500\n",
      "Epoch 241/500\n",
      " - 0s - loss: 0.8173 - accuracy: 0.5449 - val_loss: 0.7952 - val_accuracy: 0.5500\n",
      "Epoch 242/500\n",
      " - 0s - loss: 0.8145 - accuracy: 0.5577 - val_loss: 0.7953 - val_accuracy: 0.5500\n",
      "Epoch 243/500\n",
      " - 0s - loss: 0.8072 - accuracy: 0.5962 - val_loss: 0.7949 - val_accuracy: 0.5500\n",
      "Epoch 244/500\n",
      " - 0s - loss: 0.7987 - accuracy: 0.5769 - val_loss: 0.7956 - val_accuracy: 0.5500\n",
      "Epoch 245/500\n",
      " - 0s - loss: 0.8406 - accuracy: 0.5321 - val_loss: 0.7963 - val_accuracy: 0.5500\n",
      "Epoch 246/500\n",
      " - 0s - loss: 0.8140 - accuracy: 0.5513 - val_loss: 0.7959 - val_accuracy: 0.5500\n",
      "Epoch 247/500\n",
      " - 0s - loss: 0.8190 - accuracy: 0.5833 - val_loss: 0.7962 - val_accuracy: 0.5500\n",
      "Epoch 248/500\n",
      " - 0s - loss: 0.8442 - accuracy: 0.5256 - val_loss: 0.7963 - val_accuracy: 0.5500\n",
      "Epoch 249/500\n",
      " - 0s - loss: 0.8122 - accuracy: 0.5321 - val_loss: 0.7950 - val_accuracy: 0.5500\n",
      "Epoch 250/500\n",
      " - 0s - loss: 0.8324 - accuracy: 0.5513 - val_loss: 0.7954 - val_accuracy: 0.5500\n",
      "Epoch 251/500\n",
      " - 0s - loss: 0.8208 - accuracy: 0.5513 - val_loss: 0.7957 - val_accuracy: 0.5500\n",
      "Epoch 252/500\n",
      " - 0s - loss: 0.8167 - accuracy: 0.5769 - val_loss: 0.7960 - val_accuracy: 0.5500\n",
      "Epoch 253/500\n",
      " - 0s - loss: 0.8115 - accuracy: 0.5705 - val_loss: 0.7967 - val_accuracy: 0.5500\n",
      "Epoch 254/500\n",
      " - 0s - loss: 0.8377 - accuracy: 0.5769 - val_loss: 0.7971 - val_accuracy: 0.5500\n",
      "Epoch 255/500\n",
      " - 0s - loss: 0.8187 - accuracy: 0.5897 - val_loss: 0.7966 - val_accuracy: 0.5500\n",
      "Epoch 256/500\n",
      " - 0s - loss: 0.8279 - accuracy: 0.5385 - val_loss: 0.7968 - val_accuracy: 0.5500\n",
      "Epoch 257/500\n",
      " - 0s - loss: 0.8098 - accuracy: 0.5449 - val_loss: 0.7961 - val_accuracy: 0.5500\n",
      "Epoch 258/500\n",
      " - 0s - loss: 0.8342 - accuracy: 0.5449 - val_loss: 0.7954 - val_accuracy: 0.5500\n",
      "Epoch 259/500\n",
      " - 0s - loss: 0.8185 - accuracy: 0.5705 - val_loss: 0.7956 - val_accuracy: 0.5500\n",
      "Epoch 260/500\n",
      " - 0s - loss: 0.8451 - accuracy: 0.5000 - val_loss: 0.7948 - val_accuracy: 0.5500\n",
      "Epoch 261/500\n",
      " - 0s - loss: 0.8200 - accuracy: 0.5449 - val_loss: 0.7948 - val_accuracy: 0.5500\n",
      "Epoch 262/500\n",
      " - 0s - loss: 0.8284 - accuracy: 0.5897 - val_loss: 0.7934 - val_accuracy: 0.5500\n",
      "Epoch 263/500\n",
      " - 0s - loss: 0.8344 - accuracy: 0.5385 - val_loss: 0.7932 - val_accuracy: 0.5500\n",
      "Epoch 264/500\n",
      " - 0s - loss: 0.8057 - accuracy: 0.5385 - val_loss: 0.7940 - val_accuracy: 0.5500\n",
      "Epoch 265/500\n",
      " - 0s - loss: 0.8378 - accuracy: 0.5705 - val_loss: 0.7942 - val_accuracy: 0.5500\n",
      "Epoch 266/500\n",
      " - 0s - loss: 0.8211 - accuracy: 0.5641 - val_loss: 0.7943 - val_accuracy: 0.5500\n",
      "Epoch 267/500\n",
      " - 0s - loss: 0.8217 - accuracy: 0.5769 - val_loss: 0.7950 - val_accuracy: 0.5500\n",
      "Epoch 268/500\n",
      " - 0s - loss: 0.8389 - accuracy: 0.6026 - val_loss: 0.7954 - val_accuracy: 0.5500\n",
      "Epoch 269/500\n",
      " - 0s - loss: 0.8204 - accuracy: 0.5705 - val_loss: 0.7958 - val_accuracy: 0.5500\n",
      "Epoch 270/500\n",
      " - 0s - loss: 0.8096 - accuracy: 0.5897 - val_loss: 0.7964 - val_accuracy: 0.5500\n",
      "Epoch 271/500\n",
      " - 0s - loss: 0.8310 - accuracy: 0.5577 - val_loss: 0.7966 - val_accuracy: 0.5500\n",
      "Epoch 272/500\n",
      " - 0s - loss: 0.8683 - accuracy: 0.5705 - val_loss: 0.7965 - val_accuracy: 0.5500\n",
      "Epoch 273/500\n",
      " - 0s - loss: 0.8220 - accuracy: 0.5192 - val_loss: 0.7965 - val_accuracy: 0.5500\n",
      "Epoch 274/500\n",
      " - 0s - loss: 0.8535 - accuracy: 0.5705 - val_loss: 0.7961 - val_accuracy: 0.5500\n",
      "Epoch 275/500\n",
      " - 0s - loss: 0.8238 - accuracy: 0.5577 - val_loss: 0.7962 - val_accuracy: 0.5500\n",
      "Epoch 276/500\n",
      " - 0s - loss: 0.8128 - accuracy: 0.5705 - val_loss: 0.7960 - val_accuracy: 0.5500\n",
      "Epoch 277/500\n",
      " - 0s - loss: 0.8342 - accuracy: 0.5705 - val_loss: 0.7967 - val_accuracy: 0.5500\n",
      "Epoch 278/500\n",
      " - 0s - loss: 0.8235 - accuracy: 0.5641 - val_loss: 0.7974 - val_accuracy: 0.5500\n",
      "Epoch 279/500\n",
      " - 0s - loss: 0.8434 - accuracy: 0.5577 - val_loss: 0.7971 - val_accuracy: 0.5500\n",
      "Epoch 280/500\n",
      " - 0s - loss: 0.8117 - accuracy: 0.5321 - val_loss: 0.7966 - val_accuracy: 0.5500\n",
      "Epoch 281/500\n",
      " - 0s - loss: 0.7979 - accuracy: 0.5513 - val_loss: 0.7962 - val_accuracy: 0.5500\n",
      "Epoch 282/500\n",
      " - 0s - loss: 0.8254 - accuracy: 0.5833 - val_loss: 0.7958 - val_accuracy: 0.5500\n",
      "Epoch 283/500\n",
      " - 0s - loss: 0.8601 - accuracy: 0.5577 - val_loss: 0.7966 - val_accuracy: 0.5500\n",
      "Epoch 284/500\n",
      " - 0s - loss: 0.8324 - accuracy: 0.5192 - val_loss: 0.7969 - val_accuracy: 0.5500\n",
      "Epoch 285/500\n",
      " - 0s - loss: 0.8327 - accuracy: 0.5705 - val_loss: 0.7964 - val_accuracy: 0.5500\n",
      "Epoch 286/500\n",
      " - 0s - loss: 0.8366 - accuracy: 0.5577 - val_loss: 0.7955 - val_accuracy: 0.5500\n",
      "Epoch 287/500\n",
      " - 0s - loss: 0.8517 - accuracy: 0.4872 - val_loss: 0.7963 - val_accuracy: 0.5500\n",
      "Epoch 288/500\n",
      " - 0s - loss: 0.8087 - accuracy: 0.5513 - val_loss: 0.7963 - val_accuracy: 0.5500\n",
      "Epoch 289/500\n",
      " - 0s - loss: 0.8258 - accuracy: 0.5577 - val_loss: 0.7961 - val_accuracy: 0.5500\n",
      "Epoch 290/500\n",
      " - 0s - loss: 0.8641 - accuracy: 0.5256 - val_loss: 0.7967 - val_accuracy: 0.5500\n",
      "Epoch 291/500\n",
      " - 0s - loss: 0.8171 - accuracy: 0.5577 - val_loss: 0.7957 - val_accuracy: 0.5500\n",
      "Epoch 292/500\n",
      " - 0s - loss: 0.8281 - accuracy: 0.5256 - val_loss: 0.7961 - val_accuracy: 0.5500\n",
      "Epoch 293/500\n",
      " - 0s - loss: 0.8576 - accuracy: 0.5513 - val_loss: 0.7950 - val_accuracy: 0.5500\n",
      "Epoch 294/500\n",
      " - 0s - loss: 0.8042 - accuracy: 0.5641 - val_loss: 0.7951 - val_accuracy: 0.5500\n",
      "Epoch 295/500\n",
      " - 0s - loss: 0.8307 - accuracy: 0.5641 - val_loss: 0.7953 - val_accuracy: 0.5500\n",
      "Epoch 296/500\n",
      " - 0s - loss: 0.8215 - accuracy: 0.5641 - val_loss: 0.7961 - val_accuracy: 0.5500\n",
      "Epoch 297/500\n",
      " - 0s - loss: 0.8392 - accuracy: 0.5449 - val_loss: 0.7975 - val_accuracy: 0.5500\n",
      "Epoch 298/500\n",
      " - 0s - loss: 0.8069 - accuracy: 0.5513 - val_loss: 0.7984 - val_accuracy: 0.5500\n",
      "Epoch 299/500\n",
      " - 0s - loss: 0.8372 - accuracy: 0.5449 - val_loss: 0.7984 - val_accuracy: 0.5500\n",
      "Epoch 300/500\n",
      " - 0s - loss: 0.8619 - accuracy: 0.5128 - val_loss: 0.7991 - val_accuracy: 0.5500\n",
      "Epoch 301/500\n",
      " - 0s - loss: 0.8188 - accuracy: 0.5769 - val_loss: 0.7982 - val_accuracy: 0.5500\n",
      "Epoch 302/500\n",
      " - 0s - loss: 0.8511 - accuracy: 0.5321 - val_loss: 0.7979 - val_accuracy: 0.5500\n",
      "Epoch 303/500\n",
      " - 0s - loss: 0.8471 - accuracy: 0.5192 - val_loss: 0.7972 - val_accuracy: 0.5500\n",
      "Epoch 304/500\n",
      " - 0s - loss: 0.8457 - accuracy: 0.5321 - val_loss: 0.7963 - val_accuracy: 0.5500\n",
      "Epoch 305/500\n",
      " - 0s - loss: 0.8435 - accuracy: 0.5321 - val_loss: 0.7963 - val_accuracy: 0.5500\n",
      "Epoch 306/500\n",
      " - 0s - loss: 0.7831 - accuracy: 0.5897 - val_loss: 0.7948 - val_accuracy: 0.5500\n",
      "Epoch 307/500\n",
      " - 0s - loss: 0.8157 - accuracy: 0.5513 - val_loss: 0.7942 - val_accuracy: 0.5500\n",
      "Epoch 308/500\n",
      " - 0s - loss: 0.8685 - accuracy: 0.5000 - val_loss: 0.7937 - val_accuracy: 0.5500\n",
      "Epoch 309/500\n",
      " - 0s - loss: 0.8316 - accuracy: 0.5833 - val_loss: 0.7940 - val_accuracy: 0.5500\n",
      "Epoch 310/500\n",
      " - 0s - loss: 0.8398 - accuracy: 0.5513 - val_loss: 0.7944 - val_accuracy: 0.5500\n",
      "Epoch 311/500\n",
      " - 0s - loss: 0.8428 - accuracy: 0.5385 - val_loss: 0.7953 - val_accuracy: 0.5500\n",
      "Epoch 312/500\n",
      " - 0s - loss: 0.8341 - accuracy: 0.5769 - val_loss: 0.7947 - val_accuracy: 0.5500\n",
      "Epoch 313/500\n",
      " - 0s - loss: 0.8250 - accuracy: 0.5641 - val_loss: 0.7949 - val_accuracy: 0.5500\n",
      "Epoch 314/500\n",
      " - 0s - loss: 0.8176 - accuracy: 0.5128 - val_loss: 0.7943 - val_accuracy: 0.5500\n",
      "Epoch 315/500\n",
      " - 0s - loss: 0.7914 - accuracy: 0.5577 - val_loss: 0.7932 - val_accuracy: 0.5500\n",
      "Epoch 316/500\n",
      " - 0s - loss: 0.8603 - accuracy: 0.5513 - val_loss: 0.7954 - val_accuracy: 0.5500\n",
      "Epoch 317/500\n",
      " - 0s - loss: 0.8182 - accuracy: 0.5385 - val_loss: 0.7952 - val_accuracy: 0.5500\n",
      "Epoch 318/500\n",
      " - 0s - loss: 0.8378 - accuracy: 0.5192 - val_loss: 0.7950 - val_accuracy: 0.5500\n",
      "Epoch 319/500\n",
      " - 0s - loss: 0.8346 - accuracy: 0.5897 - val_loss: 0.7948 - val_accuracy: 0.5500\n",
      "Epoch 320/500\n",
      " - 0s - loss: 0.8468 - accuracy: 0.5128 - val_loss: 0.7957 - val_accuracy: 0.5500\n",
      "Epoch 321/500\n",
      " - 0s - loss: 0.7647 - accuracy: 0.5705 - val_loss: 0.7943 - val_accuracy: 0.5500\n",
      "Epoch 322/500\n",
      " - 0s - loss: 0.8427 - accuracy: 0.5705 - val_loss: 0.7949 - val_accuracy: 0.5500\n",
      "Epoch 323/500\n",
      " - 0s - loss: 0.8087 - accuracy: 0.5833 - val_loss: 0.7950 - val_accuracy: 0.5500\n",
      "Epoch 324/500\n",
      " - 0s - loss: 0.8288 - accuracy: 0.5641 - val_loss: 0.7948 - val_accuracy: 0.5500\n",
      "Epoch 325/500\n",
      " - 0s - loss: 0.8428 - accuracy: 0.5449 - val_loss: 0.7947 - val_accuracy: 0.5500\n",
      "Epoch 326/500\n",
      " - 0s - loss: 0.8528 - accuracy: 0.5449 - val_loss: 0.7945 - val_accuracy: 0.5500\n",
      "Epoch 327/500\n",
      " - 0s - loss: 0.8235 - accuracy: 0.5192 - val_loss: 0.7947 - val_accuracy: 0.5500\n",
      "Epoch 328/500\n",
      " - 0s - loss: 0.8309 - accuracy: 0.5641 - val_loss: 0.7941 - val_accuracy: 0.5500\n",
      "Epoch 329/500\n",
      " - 0s - loss: 0.8182 - accuracy: 0.5513 - val_loss: 0.7931 - val_accuracy: 0.5500\n",
      "Epoch 330/500\n",
      " - 0s - loss: 0.8096 - accuracy: 0.5769 - val_loss: 0.7925 - val_accuracy: 0.5500\n",
      "Epoch 331/500\n",
      " - 0s - loss: 0.8248 - accuracy: 0.5769 - val_loss: 0.7926 - val_accuracy: 0.5500\n",
      "Epoch 332/500\n",
      " - 0s - loss: 0.8295 - accuracy: 0.5385 - val_loss: 0.7924 - val_accuracy: 0.5500\n",
      "Epoch 333/500\n",
      " - 0s - loss: 0.8314 - accuracy: 0.5128 - val_loss: 0.7928 - val_accuracy: 0.5500\n",
      "Epoch 334/500\n",
      " - 0s - loss: 0.8246 - accuracy: 0.5321 - val_loss: 0.7925 - val_accuracy: 0.5500\n",
      "Epoch 335/500\n",
      " - 0s - loss: 0.8496 - accuracy: 0.5449 - val_loss: 0.7926 - val_accuracy: 0.5500\n",
      "Epoch 336/500\n",
      " - 0s - loss: 0.8324 - accuracy: 0.5577 - val_loss: 0.7928 - val_accuracy: 0.5500\n",
      "Epoch 337/500\n",
      " - 0s - loss: 0.8269 - accuracy: 0.5385 - val_loss: 0.7938 - val_accuracy: 0.5500\n",
      "Epoch 338/500\n",
      " - 0s - loss: 0.8344 - accuracy: 0.5769 - val_loss: 0.7939 - val_accuracy: 0.5500\n",
      "Epoch 339/500\n",
      " - 0s - loss: 0.8346 - accuracy: 0.5385 - val_loss: 0.7946 - val_accuracy: 0.5500\n",
      "Epoch 340/500\n",
      " - 0s - loss: 0.8290 - accuracy: 0.5577 - val_loss: 0.7945 - val_accuracy: 0.5500\n",
      "Epoch 341/500\n",
      " - 0s - loss: 0.8521 - accuracy: 0.5321 - val_loss: 0.7942 - val_accuracy: 0.5500\n",
      "Epoch 342/500\n",
      " - 0s - loss: 0.8025 - accuracy: 0.5577 - val_loss: 0.7937 - val_accuracy: 0.5500\n",
      "Epoch 343/500\n",
      " - 0s - loss: 0.8347 - accuracy: 0.5449 - val_loss: 0.7933 - val_accuracy: 0.5500\n",
      "Epoch 344/500\n",
      " - 0s - loss: 0.8146 - accuracy: 0.5577 - val_loss: 0.7927 - val_accuracy: 0.5500\n",
      "Epoch 345/500\n",
      " - 0s - loss: 0.8428 - accuracy: 0.5449 - val_loss: 0.7932 - val_accuracy: 0.5500\n",
      "Epoch 346/500\n",
      " - 0s - loss: 0.8226 - accuracy: 0.5769 - val_loss: 0.7942 - val_accuracy: 0.5500\n",
      "Epoch 347/500\n",
      " - 0s - loss: 0.8158 - accuracy: 0.5449 - val_loss: 0.7948 - val_accuracy: 0.5500\n",
      "Epoch 348/500\n",
      " - 0s - loss: 0.8232 - accuracy: 0.5577 - val_loss: 0.7945 - val_accuracy: 0.5500\n",
      "Epoch 349/500\n",
      " - 0s - loss: 0.8100 - accuracy: 0.5449 - val_loss: 0.7955 - val_accuracy: 0.5500\n",
      "Epoch 350/500\n",
      " - 0s - loss: 0.8152 - accuracy: 0.5577 - val_loss: 0.7948 - val_accuracy: 0.5500\n",
      "Epoch 351/500\n",
      " - 0s - loss: 0.8322 - accuracy: 0.5513 - val_loss: 0.7947 - val_accuracy: 0.5500\n",
      "Epoch 352/500\n",
      " - 0s - loss: 0.8178 - accuracy: 0.5897 - val_loss: 0.7945 - val_accuracy: 0.5500\n",
      "Epoch 353/500\n",
      " - 0s - loss: 0.8129 - accuracy: 0.5705 - val_loss: 0.7947 - val_accuracy: 0.5500\n",
      "Epoch 354/500\n",
      " - 0s - loss: 0.8385 - accuracy: 0.5385 - val_loss: 0.7941 - val_accuracy: 0.5500\n",
      "Epoch 355/500\n",
      " - 0s - loss: 0.8425 - accuracy: 0.5577 - val_loss: 0.7952 - val_accuracy: 0.5500\n",
      "Epoch 356/500\n",
      " - 0s - loss: 0.8336 - accuracy: 0.5449 - val_loss: 0.7952 - val_accuracy: 0.5500\n",
      "Epoch 357/500\n",
      " - 0s - loss: 0.8295 - accuracy: 0.5769 - val_loss: 0.7950 - val_accuracy: 0.5500\n",
      "Epoch 358/500\n",
      " - 0s - loss: 0.8472 - accuracy: 0.5641 - val_loss: 0.7946 - val_accuracy: 0.5500\n",
      "Epoch 359/500\n",
      " - 0s - loss: 0.8312 - accuracy: 0.5641 - val_loss: 0.7946 - val_accuracy: 0.5500\n",
      "Epoch 360/500\n",
      " - 0s - loss: 0.8323 - accuracy: 0.5513 - val_loss: 0.7949 - val_accuracy: 0.5500\n",
      "Epoch 361/500\n",
      " - 0s - loss: 0.8186 - accuracy: 0.5705 - val_loss: 0.7939 - val_accuracy: 0.5500\n",
      "Epoch 362/500\n",
      " - 0s - loss: 0.8355 - accuracy: 0.5577 - val_loss: 0.7947 - val_accuracy: 0.5500\n",
      "Epoch 363/500\n",
      " - 0s - loss: 0.8371 - accuracy: 0.5641 - val_loss: 0.7952 - val_accuracy: 0.5500\n",
      "Epoch 364/500\n",
      " - 0s - loss: 0.8403 - accuracy: 0.5577 - val_loss: 0.7953 - val_accuracy: 0.5500\n",
      "Epoch 365/500\n",
      " - 0s - loss: 0.7930 - accuracy: 0.5513 - val_loss: 0.7947 - val_accuracy: 0.5500\n",
      "Epoch 366/500\n",
      " - 0s - loss: 0.8326 - accuracy: 0.5641 - val_loss: 0.7938 - val_accuracy: 0.5500\n",
      "Epoch 367/500\n",
      " - 0s - loss: 0.8492 - accuracy: 0.5705 - val_loss: 0.7931 - val_accuracy: 0.5500\n",
      "Epoch 368/500\n",
      " - 0s - loss: 0.8084 - accuracy: 0.5705 - val_loss: 0.7933 - val_accuracy: 0.5500\n",
      "Epoch 369/500\n",
      " - 0s - loss: 0.8364 - accuracy: 0.5385 - val_loss: 0.7937 - val_accuracy: 0.5500\n",
      "Epoch 370/500\n",
      " - 0s - loss: 0.8348 - accuracy: 0.5449 - val_loss: 0.7932 - val_accuracy: 0.5500\n",
      "Epoch 371/500\n",
      " - 0s - loss: 0.8509 - accuracy: 0.5192 - val_loss: 0.7932 - val_accuracy: 0.5500\n",
      "Epoch 372/500\n",
      " - 0s - loss: 0.8388 - accuracy: 0.5192 - val_loss: 0.7929 - val_accuracy: 0.5500\n",
      "Epoch 373/500\n",
      " - 0s - loss: 0.8441 - accuracy: 0.5256 - val_loss: 0.7930 - val_accuracy: 0.5500\n",
      "Epoch 374/500\n",
      " - 0s - loss: 0.8358 - accuracy: 0.5641 - val_loss: 0.7935 - val_accuracy: 0.5500\n",
      "Epoch 375/500\n",
      " - 0s - loss: 0.8560 - accuracy: 0.5641 - val_loss: 0.7934 - val_accuracy: 0.5500\n",
      "Epoch 376/500\n",
      " - 0s - loss: 0.8350 - accuracy: 0.5513 - val_loss: 0.7931 - val_accuracy: 0.5500\n",
      "Epoch 377/500\n",
      " - 0s - loss: 0.8493 - accuracy: 0.5321 - val_loss: 0.7931 - val_accuracy: 0.5500\n",
      "Epoch 378/500\n",
      " - 0s - loss: 0.8194 - accuracy: 0.5513 - val_loss: 0.7931 - val_accuracy: 0.5500\n",
      "Epoch 379/500\n",
      " - 0s - loss: 0.8370 - accuracy: 0.5449 - val_loss: 0.7935 - val_accuracy: 0.5500\n",
      "Epoch 380/500\n",
      " - 0s - loss: 0.8564 - accuracy: 0.5256 - val_loss: 0.7940 - val_accuracy: 0.5500\n",
      "Epoch 381/500\n",
      " - 0s - loss: 0.8414 - accuracy: 0.5449 - val_loss: 0.7931 - val_accuracy: 0.5500\n",
      "Epoch 382/500\n",
      " - 0s - loss: 0.7994 - accuracy: 0.5513 - val_loss: 0.7931 - val_accuracy: 0.5500\n",
      "Epoch 383/500\n",
      " - 0s - loss: 0.8404 - accuracy: 0.5449 - val_loss: 0.7934 - val_accuracy: 0.5500\n",
      "Epoch 384/500\n",
      " - 0s - loss: 0.8452 - accuracy: 0.5321 - val_loss: 0.7937 - val_accuracy: 0.5500\n",
      "Epoch 385/500\n",
      " - 0s - loss: 0.8124 - accuracy: 0.5833 - val_loss: 0.7927 - val_accuracy: 0.5500\n",
      "Epoch 386/500\n",
      " - 0s - loss: 0.8413 - accuracy: 0.5449 - val_loss: 0.7928 - val_accuracy: 0.5500\n",
      "Epoch 387/500\n",
      " - 0s - loss: 0.8389 - accuracy: 0.5577 - val_loss: 0.7925 - val_accuracy: 0.5500\n",
      "Epoch 388/500\n",
      " - 0s - loss: 0.8364 - accuracy: 0.5577 - val_loss: 0.7923 - val_accuracy: 0.5500\n",
      "Epoch 389/500\n",
      " - 0s - loss: 0.8271 - accuracy: 0.5385 - val_loss: 0.7923 - val_accuracy: 0.5500\n",
      "Epoch 390/500\n",
      " - 0s - loss: 0.8146 - accuracy: 0.5449 - val_loss: 0.7925 - val_accuracy: 0.5500\n",
      "Epoch 391/500\n",
      " - 0s - loss: 0.8192 - accuracy: 0.5641 - val_loss: 0.7933 - val_accuracy: 0.5500\n",
      "Epoch 392/500\n",
      " - 0s - loss: 0.8340 - accuracy: 0.5256 - val_loss: 0.7928 - val_accuracy: 0.5500\n",
      "Epoch 393/500\n",
      " - 0s - loss: 0.8102 - accuracy: 0.5641 - val_loss: 0.7927 - val_accuracy: 0.5500\n",
      "Epoch 394/500\n",
      " - 0s - loss: 0.8056 - accuracy: 0.5833 - val_loss: 0.7929 - val_accuracy: 0.5500\n",
      "Epoch 395/500\n",
      " - 0s - loss: 0.8511 - accuracy: 0.5769 - val_loss: 0.7928 - val_accuracy: 0.5500\n",
      "Epoch 396/500\n",
      " - 0s - loss: 0.8235 - accuracy: 0.5385 - val_loss: 0.7934 - val_accuracy: 0.5500\n",
      "Epoch 397/500\n",
      " - 0s - loss: 0.8186 - accuracy: 0.5641 - val_loss: 0.7936 - val_accuracy: 0.5500\n",
      "Epoch 398/500\n",
      " - 0s - loss: 0.8316 - accuracy: 0.5641 - val_loss: 0.7936 - val_accuracy: 0.5500\n",
      "Epoch 399/500\n",
      " - 0s - loss: 0.8173 - accuracy: 0.5577 - val_loss: 0.7938 - val_accuracy: 0.5500\n",
      "Epoch 400/500\n",
      " - 0s - loss: 0.8399 - accuracy: 0.5577 - val_loss: 0.7943 - val_accuracy: 0.5500\n",
      "Epoch 401/500\n",
      " - 0s - loss: 0.8627 - accuracy: 0.5192 - val_loss: 0.7938 - val_accuracy: 0.5500\n",
      "Epoch 402/500\n",
      " - 0s - loss: 0.8540 - accuracy: 0.5513 - val_loss: 0.7946 - val_accuracy: 0.5500\n",
      "Epoch 403/500\n",
      " - 0s - loss: 0.8624 - accuracy: 0.4679 - val_loss: 0.7943 - val_accuracy: 0.5500\n",
      "Epoch 404/500\n",
      " - 0s - loss: 0.8056 - accuracy: 0.5769 - val_loss: 0.7963 - val_accuracy: 0.5500\n",
      "Epoch 405/500\n",
      " - 0s - loss: 0.8199 - accuracy: 0.5705 - val_loss: 0.7967 - val_accuracy: 0.5500\n",
      "Epoch 406/500\n",
      " - 0s - loss: 0.8382 - accuracy: 0.5513 - val_loss: 0.7963 - val_accuracy: 0.5500\n",
      "Epoch 407/500\n",
      " - 0s - loss: 0.8041 - accuracy: 0.5705 - val_loss: 0.7965 - val_accuracy: 0.5500\n",
      "Epoch 408/500\n",
      " - 0s - loss: 0.7885 - accuracy: 0.5897 - val_loss: 0.7969 - val_accuracy: 0.5500\n",
      "Epoch 409/500\n",
      " - 0s - loss: 0.8141 - accuracy: 0.5833 - val_loss: 0.7965 - val_accuracy: 0.5500\n",
      "Epoch 410/500\n",
      " - 0s - loss: 0.8450 - accuracy: 0.5192 - val_loss: 0.7969 - val_accuracy: 0.5500\n",
      "Epoch 411/500\n",
      " - 0s - loss: 0.8557 - accuracy: 0.5256 - val_loss: 0.7976 - val_accuracy: 0.5500\n",
      "Epoch 412/500\n",
      " - 0s - loss: 0.8405 - accuracy: 0.5256 - val_loss: 0.7966 - val_accuracy: 0.5500\n",
      "Epoch 413/500\n",
      " - 0s - loss: 0.8410 - accuracy: 0.5192 - val_loss: 0.7966 - val_accuracy: 0.5500\n",
      "Epoch 414/500\n",
      " - 0s - loss: 0.8250 - accuracy: 0.5897 - val_loss: 0.7973 - val_accuracy: 0.5500\n",
      "Epoch 415/500\n",
      " - 0s - loss: 0.8282 - accuracy: 0.5769 - val_loss: 0.7976 - val_accuracy: 0.5500\n",
      "Epoch 416/500\n",
      " - 0s - loss: 0.8314 - accuracy: 0.5513 - val_loss: 0.7967 - val_accuracy: 0.5500\n",
      "Epoch 417/500\n",
      " - 0s - loss: 0.8150 - accuracy: 0.5705 - val_loss: 0.7971 - val_accuracy: 0.5500\n",
      "Epoch 418/500\n",
      " - 0s - loss: 0.8537 - accuracy: 0.5256 - val_loss: 0.7972 - val_accuracy: 0.5500\n",
      "Epoch 419/500\n",
      " - 0s - loss: 0.8505 - accuracy: 0.5513 - val_loss: 0.7978 - val_accuracy: 0.5500\n",
      "Epoch 420/500\n",
      " - 0s - loss: 0.8404 - accuracy: 0.5385 - val_loss: 0.7981 - val_accuracy: 0.5500\n",
      "Epoch 421/500\n",
      " - 0s - loss: 0.8095 - accuracy: 0.5833 - val_loss: 0.7975 - val_accuracy: 0.5500\n",
      "Epoch 422/500\n",
      " - 0s - loss: 0.8182 - accuracy: 0.5577 - val_loss: 0.7968 - val_accuracy: 0.5500\n",
      "Epoch 423/500\n",
      " - 0s - loss: 0.8301 - accuracy: 0.5449 - val_loss: 0.7956 - val_accuracy: 0.5500\n",
      "Epoch 424/500\n",
      " - 0s - loss: 0.8674 - accuracy: 0.5449 - val_loss: 0.7956 - val_accuracy: 0.5500\n",
      "Epoch 425/500\n",
      " - 0s - loss: 0.8482 - accuracy: 0.5449 - val_loss: 0.7955 - val_accuracy: 0.5500\n",
      "Epoch 426/500\n",
      " - 0s - loss: 0.8407 - accuracy: 0.5577 - val_loss: 0.7947 - val_accuracy: 0.5500\n",
      "Epoch 427/500\n",
      " - 0s - loss: 0.8443 - accuracy: 0.5000 - val_loss: 0.7943 - val_accuracy: 0.5500\n",
      "Epoch 428/500\n",
      " - 0s - loss: 0.8406 - accuracy: 0.5192 - val_loss: 0.7945 - val_accuracy: 0.5500\n",
      "Epoch 429/500\n",
      " - 0s - loss: 0.8372 - accuracy: 0.5833 - val_loss: 0.7945 - val_accuracy: 0.5500\n",
      "Epoch 430/500\n",
      " - 0s - loss: 0.8253 - accuracy: 0.5705 - val_loss: 0.7937 - val_accuracy: 0.5500\n",
      "Epoch 431/500\n",
      " - 0s - loss: 0.8063 - accuracy: 0.5449 - val_loss: 0.7931 - val_accuracy: 0.5500\n",
      "Epoch 432/500\n",
      " - 0s - loss: 0.8433 - accuracy: 0.5385 - val_loss: 0.7925 - val_accuracy: 0.5500\n",
      "Epoch 433/500\n",
      " - 0s - loss: 0.8248 - accuracy: 0.5513 - val_loss: 0.7925 - val_accuracy: 0.5500\n",
      "Epoch 434/500\n",
      " - 0s - loss: 0.8350 - accuracy: 0.5449 - val_loss: 0.7922 - val_accuracy: 0.5500\n",
      "Epoch 435/500\n",
      " - 0s - loss: 0.8315 - accuracy: 0.5385 - val_loss: 0.7912 - val_accuracy: 0.5500\n",
      "Epoch 436/500\n",
      " - 0s - loss: 0.8190 - accuracy: 0.5385 - val_loss: 0.7907 - val_accuracy: 0.5500\n",
      "Epoch 437/500\n",
      " - 0s - loss: 0.8490 - accuracy: 0.5513 - val_loss: 0.7912 - val_accuracy: 0.5500\n",
      "Epoch 438/500\n",
      " - 0s - loss: 0.8281 - accuracy: 0.5513 - val_loss: 0.7914 - val_accuracy: 0.5500\n",
      "Epoch 439/500\n",
      " - 0s - loss: 0.8554 - accuracy: 0.5641 - val_loss: 0.7914 - val_accuracy: 0.5500\n",
      "Epoch 440/500\n",
      " - 0s - loss: 0.8189 - accuracy: 0.5192 - val_loss: 0.7911 - val_accuracy: 0.5500\n",
      "Epoch 441/500\n",
      " - 0s - loss: 0.8330 - accuracy: 0.5641 - val_loss: 0.7915 - val_accuracy: 0.5500\n",
      "Epoch 442/500\n",
      " - 0s - loss: 0.8481 - accuracy: 0.5321 - val_loss: 0.7916 - val_accuracy: 0.5500\n",
      "Epoch 443/500\n",
      " - 0s - loss: 0.8143 - accuracy: 0.5385 - val_loss: 0.7919 - val_accuracy: 0.5500\n",
      "Epoch 444/500\n",
      " - 0s - loss: 0.8432 - accuracy: 0.5449 - val_loss: 0.7922 - val_accuracy: 0.5500\n",
      "Epoch 445/500\n",
      " - 0s - loss: 0.8584 - accuracy: 0.5641 - val_loss: 0.7917 - val_accuracy: 0.5500\n",
      "Epoch 446/500\n",
      " - 0s - loss: 0.8321 - accuracy: 0.5128 - val_loss: 0.7920 - val_accuracy: 0.5500\n",
      "Epoch 447/500\n",
      " - 0s - loss: 0.8350 - accuracy: 0.5641 - val_loss: 0.7925 - val_accuracy: 0.5500\n",
      "Epoch 448/500\n",
      " - 0s - loss: 0.8143 - accuracy: 0.5321 - val_loss: 0.7933 - val_accuracy: 0.5500\n",
      "Epoch 449/500\n",
      " - 0s - loss: 0.8237 - accuracy: 0.5705 - val_loss: 0.7925 - val_accuracy: 0.5500\n",
      "Epoch 450/500\n",
      " - 0s - loss: 0.8389 - accuracy: 0.5449 - val_loss: 0.7921 - val_accuracy: 0.5500\n",
      "Epoch 451/500\n",
      " - 0s - loss: 0.8236 - accuracy: 0.5321 - val_loss: 0.7912 - val_accuracy: 0.5500\n",
      "Epoch 452/500\n",
      " - 0s - loss: 0.8109 - accuracy: 0.5577 - val_loss: 0.7906 - val_accuracy: 0.5500\n",
      "Epoch 453/500\n",
      " - 0s - loss: 0.8195 - accuracy: 0.5833 - val_loss: 0.7914 - val_accuracy: 0.5500\n",
      "Epoch 454/500\n",
      " - 0s - loss: 0.8353 - accuracy: 0.5385 - val_loss: 0.7923 - val_accuracy: 0.5500\n",
      "Epoch 455/500\n",
      " - 0s - loss: 0.8348 - accuracy: 0.5385 - val_loss: 0.7919 - val_accuracy: 0.5500\n",
      "Epoch 456/500\n",
      " - 0s - loss: 0.8195 - accuracy: 0.6154 - val_loss: 0.7926 - val_accuracy: 0.5500\n",
      "Epoch 457/500\n",
      " - 0s - loss: 0.8292 - accuracy: 0.5513 - val_loss: 0.7928 - val_accuracy: 0.5500\n",
      "Epoch 458/500\n",
      " - 0s - loss: 0.8212 - accuracy: 0.5577 - val_loss: 0.7929 - val_accuracy: 0.5500\n",
      "Epoch 459/500\n",
      " - 0s - loss: 0.8474 - accuracy: 0.5192 - val_loss: 0.7930 - val_accuracy: 0.5500\n",
      "Epoch 460/500\n",
      " - 0s - loss: 0.8583 - accuracy: 0.5577 - val_loss: 0.7936 - val_accuracy: 0.5500\n",
      "Epoch 461/500\n",
      " - 0s - loss: 0.8290 - accuracy: 0.5705 - val_loss: 0.7945 - val_accuracy: 0.5500\n",
      "Epoch 462/500\n",
      " - 0s - loss: 0.8047 - accuracy: 0.5769 - val_loss: 0.7948 - val_accuracy: 0.5500\n",
      "Epoch 463/500\n",
      " - 0s - loss: 0.8483 - accuracy: 0.5513 - val_loss: 0.7942 - val_accuracy: 0.5500\n",
      "Epoch 464/500\n",
      " - 0s - loss: 0.8253 - accuracy: 0.5577 - val_loss: 0.7950 - val_accuracy: 0.5500\n",
      "Epoch 465/500\n",
      " - 0s - loss: 0.8072 - accuracy: 0.5833 - val_loss: 0.7942 - val_accuracy: 0.5500\n",
      "Epoch 466/500\n",
      " - 0s - loss: 0.8408 - accuracy: 0.5128 - val_loss: 0.7945 - val_accuracy: 0.5500\n",
      "Epoch 467/500\n",
      " - 0s - loss: 0.8247 - accuracy: 0.5769 - val_loss: 0.7946 - val_accuracy: 0.5500\n",
      "Epoch 468/500\n",
      " - 0s - loss: 0.8300 - accuracy: 0.5705 - val_loss: 0.7939 - val_accuracy: 0.5500\n",
      "Epoch 469/500\n",
      " - 0s - loss: 0.8135 - accuracy: 0.5385 - val_loss: 0.7936 - val_accuracy: 0.5500\n",
      "Epoch 470/500\n",
      " - 0s - loss: 0.8043 - accuracy: 0.5769 - val_loss: 0.7936 - val_accuracy: 0.5500\n",
      "Epoch 471/500\n",
      " - 0s - loss: 0.8200 - accuracy: 0.5705 - val_loss: 0.7927 - val_accuracy: 0.5500\n",
      "Epoch 472/500\n",
      " - 0s - loss: 0.8405 - accuracy: 0.5449 - val_loss: 0.7922 - val_accuracy: 0.5500\n",
      "Epoch 473/500\n",
      " - 0s - loss: 0.8395 - accuracy: 0.5385 - val_loss: 0.7920 - val_accuracy: 0.5500\n",
      "Epoch 474/500\n",
      " - 0s - loss: 0.8447 - accuracy: 0.5577 - val_loss: 0.7923 - val_accuracy: 0.5500\n",
      "Epoch 475/500\n",
      " - 0s - loss: 0.8220 - accuracy: 0.5385 - val_loss: 0.7921 - val_accuracy: 0.5500\n",
      "Epoch 476/500\n",
      " - 0s - loss: 0.8470 - accuracy: 0.5064 - val_loss: 0.7923 - val_accuracy: 0.5500\n",
      "Epoch 477/500\n",
      " - 0s - loss: 0.8456 - accuracy: 0.5128 - val_loss: 0.7917 - val_accuracy: 0.5500\n",
      "Epoch 478/500\n",
      " - 0s - loss: 0.8178 - accuracy: 0.5577 - val_loss: 0.7907 - val_accuracy: 0.5500\n",
      "Epoch 479/500\n",
      " - 0s - loss: 0.8232 - accuracy: 0.5449 - val_loss: 0.7905 - val_accuracy: 0.5500\n",
      "Epoch 480/500\n",
      " - 0s - loss: 0.8226 - accuracy: 0.5192 - val_loss: 0.7908 - val_accuracy: 0.5500\n",
      "Epoch 481/500\n",
      " - 0s - loss: 0.8078 - accuracy: 0.5449 - val_loss: 0.7908 - val_accuracy: 0.5500\n",
      "Epoch 482/500\n",
      " - 0s - loss: 0.8275 - accuracy: 0.5449 - val_loss: 0.7908 - val_accuracy: 0.5500\n",
      "Epoch 483/500\n",
      " - 0s - loss: 0.8423 - accuracy: 0.5321 - val_loss: 0.7917 - val_accuracy: 0.5500\n",
      "Epoch 484/500\n",
      " - 0s - loss: 0.8406 - accuracy: 0.5577 - val_loss: 0.7921 - val_accuracy: 0.5500\n",
      "Epoch 485/500\n",
      " - 0s - loss: 0.7997 - accuracy: 0.5256 - val_loss: 0.7930 - val_accuracy: 0.5500\n",
      "Epoch 486/500\n",
      " - 0s - loss: 0.8379 - accuracy: 0.5385 - val_loss: 0.7926 - val_accuracy: 0.5500\n",
      "Epoch 487/500\n",
      " - 0s - loss: 0.8390 - accuracy: 0.5513 - val_loss: 0.7932 - val_accuracy: 0.5500\n",
      "Epoch 488/500\n",
      " - 0s - loss: 0.8411 - accuracy: 0.5833 - val_loss: 0.7934 - val_accuracy: 0.5500\n",
      "Epoch 489/500\n",
      " - 0s - loss: 0.8235 - accuracy: 0.5833 - val_loss: 0.7930 - val_accuracy: 0.5500\n",
      "Epoch 490/500\n",
      " - 0s - loss: 0.8221 - accuracy: 0.5577 - val_loss: 0.7942 - val_accuracy: 0.5500\n",
      "Epoch 491/500\n",
      " - 0s - loss: 0.8226 - accuracy: 0.5641 - val_loss: 0.7941 - val_accuracy: 0.5500\n",
      "Epoch 492/500\n",
      " - 0s - loss: 0.8281 - accuracy: 0.5513 - val_loss: 0.7944 - val_accuracy: 0.5500\n",
      "Epoch 493/500\n",
      " - 0s - loss: 0.8245 - accuracy: 0.6090 - val_loss: 0.7952 - val_accuracy: 0.5500\n",
      "Epoch 494/500\n",
      " - 0s - loss: 0.8240 - accuracy: 0.5321 - val_loss: 0.7948 - val_accuracy: 0.5500\n",
      "Epoch 495/500\n",
      " - 0s - loss: 0.8244 - accuracy: 0.5705 - val_loss: 0.7946 - val_accuracy: 0.5500\n",
      "Epoch 496/500\n",
      " - 0s - loss: 0.8206 - accuracy: 0.5769 - val_loss: 0.7953 - val_accuracy: 0.5500\n",
      "Epoch 497/500\n",
      " - 0s - loss: 0.8209 - accuracy: 0.5577 - val_loss: 0.7958 - val_accuracy: 0.5500\n",
      "Epoch 498/500\n",
      " - 0s - loss: 0.8479 - accuracy: 0.5192 - val_loss: 0.7958 - val_accuracy: 0.5500\n",
      "Epoch 499/500\n",
      " - 0s - loss: 0.8191 - accuracy: 0.5641 - val_loss: 0.7957 - val_accuracy: 0.5500\n",
      "Epoch 500/500\n",
      " - 0s - loss: 0.7778 - accuracy: 0.5577 - val_loss: 0.7959 - val_accuracy: 0.5500\n",
      "40/40 [==============================] - 0s 76us/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-48f9b9daedca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0ml2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mmy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mroc_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_origin_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_origin_train\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# new_record = {'Batch_size': nb, 'Learning_rate': nlr, '1st layer': l1, '2nd layer': l2,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-2cd806bf25a2>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(model, X, y, nfold, nbatch, nlr, l1, l2)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalidation_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_cat_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtprs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_fpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0maucs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# final_lr = model.optimizer.lr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()  # 시작 시간 체크\n",
    "# ==== Step 1. Load original dataset\n",
    "PATH_EXCEL = './BRC2019_CRF_merged_200717 복사본 (copy).xlsx'\n",
    "metadata = sonya.get_metadata(PATH_EXCEL)\n",
    "\n",
    "df_filtered = metadata.dropna(axis=0, how='any').reset_index(drop=True)  # NaN drop\n",
    "properties = list(df_filtered.columns.values)\n",
    "properties.remove('label')\n",
    "### >>> FILTERING <<<<<<\n",
    "properties.remove('cAverage')\n",
    "properties.remove('cSD')\n",
    "properties.remove('aAverage')\n",
    "properties.remove('aSD')\n",
    "properties.remove('non-mass')\n",
    "properties.remove('rim')\n",
    "properties.remove('hetero')\n",
    "properties.remove('clustered')\n",
    "properties.remove('age')\n",
    "properties.remove('ID')\n",
    "properties.remove('L/R')\n",
    "properties.remove('LR')\n",
    "X_origin = df_filtered[properties]\n",
    "y_origin = df_filtered['label']\n",
    "num_features = len(X_origin.columns)\n",
    "\n",
    "print(X_origin)\n",
    "print(properties)\n",
    "\n",
    "X_origin_train, X_origin_test, y_origin_train, y_origin_test = train_test_split(X_origin, y_origin, test_size=0.3, random_state=0)\n",
    "\n",
    "record_count = 0\n",
    "\n",
    "# batch_size = [20]\n",
    "# learning_rate = [0.05, 0.001]\n",
    "# layer1 = [7, 9, 11]\n",
    "# layer2 = [9, 11]\n",
    "batch_size = 20\n",
    "nlr = 0.001\n",
    "l1 = 15\n",
    "l2 = 15\n",
    "my_model = mlp_model(num_features, lr=nlr, l1=l1, l2=l2)\n",
    "roc_result = cross_validation(my_model, X_origin_train, y_origin_train,  nbatch=batch_size, nlr=nlr, l1=l1, l2=l2)\n",
    "\n",
    "# new_record = {'Batch_size': nb, 'Learning_rate': nlr, '1st layer': l1, '2nd layer': l2,\n",
    "#               \"num_input\": len(_X.columns), \"input_params\": _X.columns.values, \"mean_ROC\": roc_result}\n",
    "# record.loc[i] = new_record\n",
    "\n",
    "\n",
    "## ------------ TEST_Sensitivity --------------\n",
    "# predIdxs = my_model.evaluate(X_origin_test,y_origin_test)\n",
    "loss, accuracy = my_model.evaluate(X_origin_test,y_origin_test, steps=len(X_origin_test)//batch_size)\n",
    "# predIdxs2 = my_model.predict(x=X_origin_test, steps=None)\n",
    "print(\"============================================================\")\n",
    "print(\"test loss     : {:7.4f}\".format(loss))\n",
    "print(\"test accuracy : {:7.4f}\".format(accuracy))\n",
    "# print(predIdxs)\n",
    "print(\"============================================================\")\n",
    "# print(predIdxs2)\n",
    "# # predIdxs = my_model.predict(x=X_origin_test, steps=(len(_X) // BS) + 1)\n",
    "# # predIdxs = my_model.predict(x=X_origin_test, steps=(totalTest // BS) + 1)\n",
    "\n",
    "# # predIdxs = np.argmax(predIdxs, axis=1)\n",
    "# # cm = confusion_matrix(y_origin_test, predIdxs)\n",
    "# # cm2 = confusion_matrix(y_origin_test, predIdxs).ravel()\n",
    "# # print(cm)\n",
    "# # print(cm2)\n",
    "# # # total = sum(sum(int(cm)))\n",
    "# # # acc = (cm[0, 0] + cm[1, 1]) / total\n",
    "# # sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "# # specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "# #\n",
    "# # print(cm)\n",
    "# # print(cm2)\n",
    "# # # print(\"acc: {:.4f}\".format(acc))\n",
    "# # print(\"sensitivity: {:.4f}\".format(sensitivity))\n",
    "# # print(\"specificity: {:.4f}\".format(specificity))\n",
    "\n",
    "\n",
    "# ## --------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 59us/step\n",
      "test loss     : 0.39808998931021916\n",
      "test accuracy : 0.7976190447807312\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = my_model.evaluate(X_origin_test,y_origin_test)\n",
    "\n",
    "print(\"test loss     : {}\".format(loss))\n",
    "print(\"test accuracy : {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 51us/step\n",
      "test loss     : 0.6417768029939561\n",
      "test accuracy : 0.6309523582458496\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss, accuracy = my_model.evaluate(X_origin_test,y_origin_test)\n",
    "print(\"test loss     : {}\".format(loss))\n",
    "print(\"test accuracy : {}\".format(accuracy))\n",
    "y_pred_proba = my_model.predict_proba(X_origin_test)\n",
    "fpr, tpr, _ = roc_curve(y_origin_test,  y_pred_proba)\n",
    "\n",
    "auc = roc_auc_score(y_origin_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"auc={:.3f}, accuracy: {:.3f}\".format(auc, accuracy))\n",
    "plt.legend(loc=4)\n",
    "plt.savefig(\"./t_test/\"+str(\"4 input accuracy.png\"), transparent=False) ## save to png\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3_tf1",
   "language": "python",
   "name": "env3_tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
